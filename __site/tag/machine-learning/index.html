<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/hypertext.css">
<link rel="icon" href="/assets/favicon.png">

  <title>Tag: machine-learning</title>
</head>
<body>
  <header>
  <h1 style="color:#283747">computer science</h1>
  <nav>
    <a href="/" class="current">Tags</a>
  | <a href="/blog/" >Notes</a>
  <hr/>
  </nav>
</header>

  <div class="" style="padding-left: 12%;;max-width:720px">
    <h2>Tag: machine-learning</h2>
    <ul><li><a href="/blog/conc_ineq/config_conc/">Uniform Sampling with Concentration Bounds for Configuration Enumeration</a></li><li><a href="/blog/machine-learning/ste/">Straight-Through Estimator (STE)</a></li><li><a href="/blog/machine-learning/flat_minima/">Unconventional Approaches to Interpretability in Flat Minima</a></li><li><a href="/blog/conc_ineq/log_sample/">Understanding ε² and Logarithmic Sample Complexity</a></li><li><a href="/blog/conc_ineq/conc_design/">Design Patterns for Concentration Inequalities in Sample Complexity</a></li><li><a href="/blog/conc_ineq/understand_freq/">Understanding "Frequency" in ε² and Logarithmic Sample Complexity</a></li><li><a href="/blog/machine-learning/regularization_at_layer/">Regularizing Final vs First Layer Embeddings</a></li><li><a href="/blog/machine-learning/loss_log_scale/">Loss Functions for Log-Scale Regression</a></li><li><a href="/blog/machine-learning/group_equivariance/">Group Equivariance in Neural Networks</a></li><li><a href="/blog/conc_ineq/config_conc2/">Uniform Sampling with Concentration Bounds for Configuration Enumeration - II</a></li><li><a href="/blog/sparsity/learn_l0/">Learning Sparse Neural Networks Through L0 Regularization</a></li><li><a href="/blog/data_processing/log_transform_tradeoffs/">Understanding Log Transform Trade-offs in Regression</a></li><li><a href="/blog/conc_ineq/landau/">Understanding Landau Notation in Concentration Inequalities</a></li><li><a href="/blog/machine-learning/filter_diversity/">Novel Methods for CNN Filter Diversity</a></li><li><a href="/blog/machine-learning/spatial_locality/">Spatial Locality as Inductive Bias</a></li><li><a href="/blog/machine-learning/gumbel_softmax_trick/">The Gumbel-Softmax Trick </a></li><li><a href="/blog/probabilistic_models/bayesian_net/">A Conversational Guide to Bayesian Networks</a></li><li><a href="/blog/machine-learning/sparsity_trade_off/">Is the Sparsity-Accuracy Tradeoff Worth It?</a></li><li><a href="/blog/machine-learning/eff_net/">EfficientNet: Compound Scaling & MBConv Blocks - Complete Guide</a></li><li><a href="/blog/machine-learning/gnn/">Graph Neural Networks: Step-by-Step with Simple Examples</a></li><li><a href="/blog/machine-learning/sharpness_aware/">Sharpness-Aware Minimization (SAM) - A Mathematical & Intuitive Guide</a></li><li><a href="/blog/machine-learning/gating_activation/">Self-gating activation functions</a></li></ul>
    <div class="page-foot">
    Contact me by <a href="mailto:skchu@wustl.edu">E-mail</a> | <a href="https://github.com/kchu25">Github</a> | <a href="https://www.linkedin.com/in/kchu1/">Linkedin</a>
    <br>
    This work is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>. 
    <br>
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia language</a>.
</div>

  </div>
</body>
</html>
