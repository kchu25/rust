<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/hypertext.css">
<link rel="icon" href="/assets/favicon.png">

   <title>From DeepLIFT Contributions to Shapley Values: The Quantitative Connection</title>  
</head>
<body>
<header>
  <h1 style="color:#283747">computer science</h1>
  <nav>
    <a href="/" class="current">Tags</a>
  | <a href="/blog/" >Notes</a>
  <hr/>
  </nav>
</header>


<!-- Content appended here -->
<div class="franklin-content"><h1 id="from_deeplift_contributions_to_shapley_values_the_quantitative_connection"><a href="#from_deeplift_contributions_to_shapley_values_the_quantitative_connection" class="header-anchor">From DeepLIFT Contributions to Shapley Values: The Quantitative Connection</a></h1>
<h2 id="the_core_question"><a href="#the_core_question" class="header-anchor">The Core Question</a></h2>
<p>How does averaging DeepLIFT contribution scores over multiple reference inputs produce SHAP &#40;Shapley&#41; values? Let&#39;s make this connection mathematically explicit.</p>
<h2 id="step_1_what_deeplift_gives_you"><a href="#step_1_what_deeplift_gives_you" class="header-anchor">Step 1: What DeepLIFT Gives You</a></h2>
<p>For a single input \(x\) and reference \(r\), DeepLIFT computes contribution scores:</p>
\[C_{\Delta x_i \Delta f}(x, r) = \phi_i(x, r) \cdot (x_i - r_i)\]
<p>These satisfy the <strong>summation-to-delta</strong> property:</p>
\[f(x) - f(r) = \sum_{i=1}^{n} C_{\Delta x_i \Delta f}(x, r) = \sum_{i=1}^{n} \phi_i(x, r) \cdot (x_i - r_i)\]
<p>The coefficients \(\phi_i(x, r)\) depend on both the input and reference, and are computed via the multiplier backward pass through the network.</p>
<h2 id="step_2_the_shapley_value_definition"><a href="#step_2_the_shapley_value_definition" class="header-anchor">Step 2: The Shapley Value Definition</a></h2>
<p>The true Shapley value for feature \(i\) is:</p>
\[\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!(|F|-|S|-1)!}{|F|!} [f_x(S \cup \{i\}) - f_x(S)]\]
<p>where:</p>
<ul>
<li><p>\(F\) is the set of all features</p>
</li>
<li><p>\(S\) is a coalition &#40;subset&#41; not containing \(i\)</p>
</li>
<li><p>\(f_x(S)\) is the model&#39;s expected output when features in \(S\) take values from \(x\), and features not in \(S\) are &quot;missing&quot; &#40;integrated out&#41;</p>
</li>
</ul>
<p>This can be rewritten as an expectation:</p>
\[\phi_i = \mathbb{E}_{S \sim \pi}[f_x(S \cup \{i\}) - f_x(S)]\]
<p>where \(\pi\) is the uniform distribution over all possible feature coalitions.</p>
<h2 id="step_3_the_reference_distribution_approximation"><a href="#step_3_the_reference_distribution_approximation" class="header-anchor">Step 3: The Reference Distribution Approximation</a></h2>
<p><strong>Key insight:</strong> Define the function \(f_x(S)\) using a reference distribution \(\mathcal{R}\):</p>
\(f_x(S) = \mathbb{E}_{r \sim \mathcal{R}}[f(x_S, r_{\bar{S}})]\)
<p><strong>What this notation means:</strong></p>
<ul>
<li><p>\(S\) is a <strong>coalition</strong> &#61; a subset of features &#40;e.g., \(S = \{1, 3, 5\}\) means features 1, 3, and 5&#41;</p>
</li>
<li><p>\(\bar{S}\) is the <strong>complement</strong> &#61; all features NOT in \(S\)</p>
</li>
<li><p>\(x_S\) &#61; use values from input \(x\) for features in coalition \(S\)</p>
</li>
<li><p>\(r_{\bar{S}}\) &#61; use values from reference \(r\) for features NOT in \(S\)</p>
</li>
</ul>
<p><strong>Concrete example:</strong> If you have 4 features and \(S = \{2, 4\}\):</p>
<ul>
<li><p>\(f(x_S, r_{\bar{S}}) = f(r_1, x_2, r_3, x_4)\)</p>
</li>
<li><p>Features 2 and 4 come from \(x\) &#40;they&#39;re &quot;present&quot; in the coalition&#41;</p>
</li>
<li><p>Features 1 and 3 come from \(r\) &#40;they&#39;re &quot;absent&quot; from the coalition&#41;</p>
</li>
</ul>
<p>This hybrid input lets us measure: <em>&quot;What&#39;s the model output when only features in \(S\) are active?&quot;</em></p>
<p>With this definition, the Shapley value becomes:</p>
\(\phi_i = \mathbb{E}_{S \sim \pi}\mathbb{E}_{r \sim \mathcal{R}}[f(x_{S \cup \{i\}}, r_{\overline{S \cup \{i\}}}) - f(x_S, r_{\bar{S}})]\)
<p><strong>What this measures:</strong> The marginal contribution of adding feature \(i\) to coalition \(S\):</p>
<ul>
<li><p><strong>First term:</strong> \(f(x_{S \cup \{i\}}, r_{\overline{S \cup \{i\}}})\) &#61; output when coalition \(S\) AND feature \(i\) are present</p>
</li>
<li><p><strong>Second term:</strong> \(f(x_S, r_{\bar{S}})\) &#61; output when only coalition \(S\) is present &#40;feature \(i\) is absent&#41;</p>
</li>
<li><p><strong>Difference:</strong> How much does feature \(i\) change the output, given that \(S\) is already present?</p>
</li>
</ul>
<p>We average this marginal contribution over ALL possible coalitions \(S\) &#40;weighted by the Shapley formula&#41;.</p>
<h2 id="step_4_the_independence_assumption"><a href="#step_4_the_independence_assumption" class="header-anchor">Step 4: The Independence Assumption</a></h2>
<p><strong>Critical assumption:</strong> If features are conditionally independent given the reference distribution \(\mathcal{R}\), then under certain regularity conditions on the model \(f\):</p>
\(\mathbb{E}_{S \sim \pi}[f(x_{S \cup \{i\}}, r_{\overline{S \cup \{i\}}}) - f(x_S, r_{\bar{S}})] \approx \frac{f(x) - f(r)}{x_i - r_i} \cdot (x_i - r_i) = C_{\Delta x_i \Delta f}(x, r)\)
<p><strong>What&#39;s happening here:</strong></p>
<ul>
<li><p><strong>Left side:</strong> We&#39;re computing the marginal effect of feature \(i\) by:</p>
<ol>
<li><p>Taking ALL possible coalitions \(S\) </p>
</li>
<li><p>For each coalition, creating a hybrid input where \(S\) comes from \(x\) and non-\(S\) comes from \(r\)</p>
</li>
<li><p>Measuring how much feature \(i\) contributes when added to that coalition</p>
</li>
<li><p>Averaging over all coalitions</p>
</li>
</ol>
</li>
<li><p><strong>Right side:</strong> DeepLIFT computes a single coefficient \(\frac{f(x) - f(r)}{x_i - r_i}\) that linearizes the full change from \(r\) to \(x\)</p>
</li>
</ul>
<p><strong>The approximation:</strong> Under the independence assumption, these two approaches give similar results&#33; DeepLIFT&#39;s linearization implicitly captures the average marginal effect across coalitions, even though it only does one forward-backward pass instead of enumerating all \(2^n\) coalitions.</p>
<h2 id="step_5_the_averaging_step_deepshap"><a href="#step_5_the_averaging_step_deepshap" class="header-anchor">Step 5: The Averaging Step &#40;DeepSHAP&#41;</a></h2>
<p>DeepSHAP takes the expectation over the reference distribution:</p>
\[\phi_i^{\text{DeepSHAP}}(x) = \mathbb{E}_{r \sim \mathcal{R}}[C_{\Delta x_i \Delta f}(x, r)]\]
\[= \frac{1}{|R|} \sum_{r \in R} \phi_i(x, r) \cdot (x_i - r_i)\]
<p>where \(R\) is a finite sample from \(\mathcal{R}\).</p>
<h2 id="step_6_why_this_equals_the_shapley_value"><a href="#step_6_why_this_equals_the_shapley_value" class="header-anchor">Step 6: Why This Equals the Shapley Value</a></h2>
<p>Combining steps 3-5:</p>
\(\phi_i^{\text{DeepSHAP}} = \mathbb{E}_{r \sim \mathcal{R}}[C_{\Delta x_i \Delta f}(x, r)]\)
\(\approx \mathbb{E}_{r \sim \mathcal{R}}\mathbb{E}_{S \sim \pi}[f(x_{S \cup \{i\}}, r_{\overline{S \cup \{i\}}}) - f(x_S, r_{\bar{S}})]\)
\(= \mathbb{E}_{S \sim \pi}\mathbb{E}_{r \sim \mathcal{R}}[f(x_{S \cup \{i\}}, r_{\overline{S \cup \{i\}}}) - f(x_S, r_{\bar{S}})]\)
\(= \phi_i^{\text{Shapley}}\)
<p><strong>Reading this chain of equalities:</strong></p>
<ol>
<li><p><strong>Line 1:</strong> DeepSHAP &#61; average DeepLIFT contributions over references</p>
</li>
<li><p><strong>Line 2:</strong> Each DeepLIFT contribution approximates the coalition-averaged marginal effect &#40;from Step 4&#41;</p>
</li>
<li><p><strong>Line 3:</strong> Swap the order of expectations &#40;valid since they&#39;re independent&#41;</p>
</li>
<li><p><strong>Line 4:</strong> This is exactly the Shapley value definition&#33;</p>
</li>
</ol>
<p>The key step is recognizing that DeepLIFT&#39;s linearization &#40;via multipliers&#41; effectively performs the coalition sampling that the Shapley formula requires, but does it through the network structure rather than explicit enumeration of \(2^n\) coalitions.</p>
<h2 id="the_mathematical_machinery"><a href="#the_mathematical_machinery" class="header-anchor">The Mathematical Machinery</a></h2>
<h3 id="how_multipliers_enable_coalition_sampling"><a href="#how_multipliers_enable_coalition_sampling" class="header-anchor">How Multipliers Enable Coalition Sampling</a></h3>
<p>The multipliers \(m_{\Delta s_j \Delta t}\) computed by DeepLIFT satisfy:</p>
\[C_{\Delta x_i \Delta t} = \sum_{j} C_{\Delta x_i \Delta s_j} \cdot m_{\Delta s_j \Delta t}\]
<p>When you vary the reference \(r\), you&#39;re implicitly varying which features are &quot;present&quot; &#40;from \(x\)&#41; vs &quot;absent&quot; &#40;from \(r\)&#41; at each layer. The multiplier chain rule automatically accounts for all possible paths through the network, which corresponds to marginalizing over different feature coalitions.</p>
<h3 id="the_linearization_trick"><a href="#the_linearization_trick" class="header-anchor">The Linearization Trick</a></h3>
<p>For any \((x, r)\) pair, DeepLIFT finds coefficients such that:</p>
\[f(x) - f(r) = \sum_{i=1}^{n} \phi_i(x, r) \cdot (x_i - r_i)\]
<p>This is exact for this pair, but is a <em>linear</em> relationship. When we average over many references:</p>
\[\mathbb{E}_r[\phi_i(x, r) \cdot (x_i - r_i)] \approx \text{true marginal effect of } x_i\]
<p>The averaging smooths out the local linearizations into a global approximation of the Shapley value.</p>
<h2 id="practical_implementation"><a href="#practical_implementation" class="header-anchor">Practical Implementation</a></h2>
<pre><code class="language-julia">For each input x:
    Initialize: Φ_i &#61; 0 for all features i
    
    For each reference sample r in R:
        # Forward passes
        activations_x &#61; forward&#40;x&#41;
        activations_r &#61; forward&#40;r&#41;
        
        # Compute differences
        Δs_j &#61; activations_x&#91;j&#93; - activations_r&#91;j&#93;
        
        # Backward pass with multipliers
        contributions &#61; backprop_multipliers&#40;Δs, network&#41;
        
        # Accumulate
        Φ_i &#43;&#61; contributions&#91;i&#93;
    
    # Average
    Φ_i &#61; Φ_i / |R|
    
    Return: Shapley values Φ</code></pre>
<h2 id="why_averaging_works_intuition"><a href="#why_averaging_works_intuition" class="header-anchor">Why Averaging Works: Intuition</a></h2>
<ul>
<li><p><strong>Single reference:</strong> DeepLIFT gives you \(\phi_i(x, r)\) that explains the difference \(f(x) - f(r)\) as a linear function of input differences.</p>
</li>
<li><p><strong>Multiple references:</strong> Different references \(r_1, r_2, \ldots\) represent different &quot;baseline contexts.&quot; Averaging over them gives you the expected marginal contribution of feature \(i\) across all possible contexts.</p>
</li>
<li><p><strong>Shapley connection:</strong> The Shapley formula \(\sum_S \text{weight}(S) \cdot [f(S \cup \{i\}) - f(S)]\) is also an average of marginal contributions across contexts &#40;coalitions \(S\)&#41;.</p>
</li>
<li><p><strong>The magic:</strong> DeepLIFT&#39;s linearization &#43; reference averaging implicitly performs the coalition sampling that Shapley requires, but does it through the network structure rather than explicit enumeration.</p>
</li>
</ul>
<h2 id="a_critical_clarification_how_references_and_coalitions_interact"><a href="#a_critical_clarification_how_references_and_coalitions_interact" class="header-anchor">A Critical Clarification: How References and Coalitions Interact</a></h2>
<p><strong>The key distinction:</strong></p>
<ul>
<li><p>The <strong>coalition \(S\)</strong> determines WHICH features come from \(x\) vs \(r\)</p>
</li>
<li><p>The <strong>reference \(r\)</strong> is randomly sampled and used for ALL features not in \(S\)</p>
</li>
</ul>
<p><strong>Example walkthrough</strong> with 3 features:</p>
<p>For coalition \(S = \{1\}\) &#40;only feature 1 is present&#41;:</p>
<ul>
<li><p>Sample \(r^{(1)} = (5, 7, 2)\), evaluate \(f(x_1, r^{(1)}_2, r^{(1)}_3) = f(x_1, 7, 2)\)</p>
</li>
<li><p>Sample \(r^{(2)} = (3, 9, 1)\), evaluate \(f(x_1, r^{(2)}_2, r^{(2)}_3) = f(x_1, 9, 1)\)</p>
</li>
<li><p>Average: \(f_x(S=\{1\}) = \mathbb{E}_r[f(x_1, r_2, r_3)]\)</p>
</li>
</ul>
<p>Notice: Feature 1 ALWAYS uses \(x_1\), but features 2 and 3 vary with each random \(r\) sample.</p>
<p><strong>Why this matters:</strong> By averaging over \(r\), we&#39;re integrating out &#40;marginalizing&#41; the variability of absent features, which gives us the expected output when only coalition \(S\) is active. This is exactly what the Shapley formula requires&#33;</p>
<h2 id="the_approximation_quality"><a href="#the_approximation_quality" class="header-anchor">The Approximation Quality</a></h2>
<p>The approximation \(\phi_i^{\text{DeepSHAP}} \approx \phi_i^{\text{Shapley}}\) is exact when:</p>
<ol>
<li><p>The model \(f\) is linear</p>
</li>
<li><p>Features are independent given \(\mathcal{R}\)</p>
</li>
<li><p>Sufficient references are sampled</p>
</li>
</ol>
<p>For nonlinear networks, it&#39;s an approximation that trades accuracy for computational efficiency:</p>
<ul>
<li><p><strong>True Shapley:</strong> \(2^n\) model evaluations</p>
</li>
<li><p><strong>DeepSHAP:</strong> \(|R|\) forward-backward passes &#40;typically \(|R| \ll 2^n\)&#41;</p>
</li>
</ul>
<p>The approximation improves as \(|R|\) increases, but there are no formal convergence guarantees—the quality depends on the reference distribution, network smoothness, and degree of feature interactions.</p>
<div class="page-foot">
    Contact me by <a href="mailto:skchu@wustl.edu">E-mail</a> | <a href="https://github.com/kchu25">Github</a> | <a href="https://www.linkedin.com/in/kchu1/">Linkedin</a>
    <br>
    This work is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.  Last modified: December 20, 2025.
    <br>
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
