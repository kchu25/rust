<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
  
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/hypertext.css">
<link rel="icon" href="/assets/favicon.png">

   <title>Making the SHAP Approximation Intuitive</title>  
</head>
<body>
<header>
  <h1 style="color:#283747">computer science</h1>
  <nav>
    <a href="/" class="current">Tags</a>
  | <a href="/blog/" >Notes</a>
  <hr/>
  </nav>
</header>


<!-- Content appended here -->
<div class="franklin-content"><h1 id="making_the_shap_approximation_intuitive"><a href="#making_the_shap_approximation_intuitive" class="header-anchor">Making the SHAP Approximation Intuitive</a></h1>
<p>Hey&#33; Great question - this is exactly where most people get confused about DeepSHAP. Let me break this down conversationally.</p>
<h2 id="the_core_confusion"><a href="#the_core_confusion" class="header-anchor">The Core Confusion</a></h2>
<p>You&#39;re right to be puzzled&#33; On one hand, we have:</p>
<ul>
<li><p><strong>Shapley values</strong>: average over exponentially many marginal contributions &#40;all coalitions \(S\)&#41;</p>
</li>
<li><p><strong>DeepLIFT/DeepSHAP</strong>: one forward-backward pass giving us coefficients \(\phi_i(x,r)\)</p>
</li>
</ul>
<p>How can one backprop approximate an exponential average? The answer is subtle and relies entirely on that independence assumption.</p>
<h2 id="whats_actually_happening_the_key_insight"><a href="#whats_actually_happening_the_key_insight" class="header-anchor">What&#39;s Actually Happening: The Key Insight</a></h2>
<p>Here&#39;s the crucial realization: <strong>under independence, all those different marginal contributions collapse to the same thing</strong>.</p>
<p>Let me explain what I mean.</p>
<h3 id="the_shapley_formula_what_we_want"><a href="#the_shapley_formula_what_we_want" class="header-anchor">The Shapley Formula &#40;What We Want&#41;</a></h3>
<p>The true Shapley value for feature \(i\) is:</p>
\(\phi_i^{\text{Shapley}} = \mathbb{E}_{S \sim \pi} \mathbb{E}_{r \sim \mathcal{R}}[f(x_{S \cup \{i\}}, r_{\overline{S \cup \{i\}}}) - f(x_S, r_{\bar{S}})]\)
<p>This has <strong>two</strong> expectations:</p>
<ol>
<li><p><strong>Outer expectation over coalitions \(S\)</strong>: average over all possible subsets</p>
</li>
<li><p><strong>Inner expectation over references \(r\)</strong>: for each coalition, average over reference samples</p>
</li>
</ol>
<p>For each coalition \(S\) and reference \(r\):</p>
<ul>
<li><p>Keep features in \(S\) at their input values \(x_S\)</p>
</li>
<li><p>Set other features to reference values \(r_{\bar{S}}\)</p>
</li>
<li><p>Measure what happens when we add feature \(i\)</p>
</li>
</ul>
<p><strong>Example with 3 features:</strong> For feature 1, we&#39;d compute:</p>
<ul>
<li><p>\(S = \emptyset\): for each reference \(r\), effect of adding \(x_1\) to \((r_1, r_2, r_3)\) → \((x_1, r_2, r_3)\)</p>
</li>
<li><p>\(S = \{2\}\): for each reference \(r\), effect of adding \(x_1\) to \((r_1, x_2, r_3)\) → \((x_1, x_2, r_3)\)</p>
</li>
<li><p>\(S = \{3\}\): for each reference \(r\), effect of adding \(x_1\) to \((r_1, r_2, x_3)\) → \((x_1, r_2, x_3)\)</p>
</li>
<li><p>\(S = \{2,3\}\): for each reference \(r\), effect of adding \(x_1\) to \((r_1, x_2, x_3)\) → \((x_1, x_2, x_3)\)</p>
</li>
</ul>
<p>For each coalition, we average over multiple reference samples, then average across coalitions.</p>
<h3 id="what_deeplift_computes_what_we_get"><a href="#what_deeplift_computes_what_we_get" class="header-anchor">What DeepLIFT Computes &#40;What We Get&#41;</a></h3>
<p>DeepLIFT does one backward pass along the path from \(r\) to \(x\) and gives us:</p>
\[f(x) - f(r) = \sum_{i=1}^n \phi_i(x,r) \cdot (x_i - r_i)\]
<p>This decomposes the <strong>total change</strong> \(f(x) - f(r)\) into feature contributions. The coefficient \(\phi_i(x,r)\) for feature \(i\) comes from integrating the gradient along the straight-line path:</p>
\[\phi_i(x,r) = \int_0^1 \frac{\partial f}{\partial x_i}(r + t(x-r)) \, dt\]
<p>This is computing: &quot;as we move from \(r\) to \(x\) &#40;varying ALL features simultaneously&#41;, how much does the gradient w.r.t. feature \(i\) contribute?&quot;</p>
<h2 id="the_magic_of_independence"><a href="#the_magic_of_independence" class="header-anchor">The Magic of Independence</a></h2>
<p>Here&#39;s where it all comes together. The conditional independence assumption actually makes <strong>both</strong> expectations simplify dramatically.</p>
<h3 id="what_conditional_independence_really_means"><a href="#what_conditional_independence_really_means" class="header-anchor">What Conditional Independence Really Means</a></h3>
<p>When we say features are conditionally independent given reference distribution \(\mathcal{R}\), we mean:</p>
\(p(r_i, r_j) = p(r_i) \cdot p(r_j) \text{ for all } i, j\)
<p>The reference features are sampled independently. This seems innocent, but it has profound consequences&#33;</p>
<h3 id="step_1_within_a_single_reference"><a href="#step_1_within_a_single_reference" class="header-anchor">Step 1: Within a Single Reference</a></h3>
<p>Let&#39;s use the Fundamental Theorem of Calculus for any coalition \(S\) and reference \(r\).</p>
<p>When we add feature \(i\) to coalition \(S\), we can write:</p>
\[f(x_{S \cup \{i\}}, r_{\overline{S \cup \{i\}}}) - f(x_S, r_{\bar{S}}) = \int_0^1 \frac{\partial f}{\partial x_i}(x_S, r_i + t(x_i - r_i), r_{\overline{S \cup \{i\}}}) \, dt \cdot (x_i - r_i)\]
<p>This says: the marginal effect is the integral of the gradient as we vary \(x_i\) from \(r_i\) to \(x_i\), <strong>keeping the coalition \(S\) fixed</strong>.</p>
<h3 id="step_2_the_independence_assumption_makes_coalitions_irrelevant"><a href="#step_2_the_independence_assumption_makes_coalitions_irrelevant" class="header-anchor">Step 2: The Independence Assumption Makes Coalitions Irrelevant</a></h3>
<p><strong>The gradient \(\frac{\partial f}{\partial x_i}\) doesn&#39;t depend on which coalition you&#39;re in.</strong></p>
<p>Mathematically:</p>
\(\frac{\partial f}{\partial x_i}(x_S, z_i, r_{\overline{S \cup \{i\}}}) \approx \frac{\partial f}{\partial x_i}(r_1, \ldots, r_{i-1}, z_i, r_{i+1}, \ldots, r_n)\)
<p><strong>In words:</strong> The effect of changing feature \(i\) is the same whether other features are at \(x\) or \(r\)&#33;</p>
<h3 id="step_3_collapsing_the_coalition_average_for_fixed_r"><a href="#step_3_collapsing_the_coalition_average_for_fixed_r" class="header-anchor">Step 3: Collapsing the Coalition Average &#40;for Fixed \(r\)&#41;</a></h3>
<p>If the gradient doesn&#39;t depend on \(S\), then for a <strong>fixed reference</strong> \(r\):</p>
\(\mathbb{E}_{S \sim \pi}\left[\int_0^1 \frac{\partial f}{\partial x_i}(x_S, r_i + t(x_i - r_i), r_{\overline{S \cup \{i\}}}) \, dt\right]\)
<p>becomes just:</p>
\(\int_0^1 \frac{\partial f}{\partial x_i}(r_1, \ldots, r_{i-1}, r_i + t(x_i - r_i), r_{i+1}, \ldots, r_n) \, dt\)
<p>We can pull the integral out of the coalition expectation because it no longer depends on \(S\)&#33;</p>
<h3 id="step_4_connecting_to_deeplift_still_for_fixed_r"><a href="#step_4_connecting_to_deeplift_still_for_fixed_r" class="header-anchor">Step 4: Connecting to DeepLIFT &#40;Still for Fixed \(r\)&#41;</a></h3>
<p>But wait - this is <strong>still not quite</strong> what DeepLIFT computes. For a given reference \(r\), DeepLIFT integrates along the full path from \(r\) to \(x\):</p>
\(\phi_i(x,r) = \int_0^1 \frac{\partial f}{\partial x_i}(r + t(x-r)) \, dt\)
<p>The difference is:</p>
<ul>
<li><p><strong>Coalition integral &#40;single-feature path&#41;</strong>: only varies feature \(i\), others stay at \(r\)</p>
</li>
<li><p><strong>DeepLIFT integral &#40;multi-feature path&#41;</strong>: varies ALL features from \(r\) to \(x\)</p>
</li>
</ul>
<h3 id="step_5_why_these_are_approximately_equal"><a href="#step_5_why_these_are_approximately_equal" class="header-anchor">Step 5: Why These Are Approximately Equal</a></h3>
<p>Under independence, the gradient \(\frac{\partial f}{\partial x_i}\) doesn&#39;t depend on the values of OTHER features &#40;no interaction terms&#41;. So:</p>
\(\frac{\partial f}{\partial x_i}(r_1, \ldots, r_{i-1}, z_i, r_{i+1}, \ldots, r_n) \approx \frac{\partial f}{\partial x_i}(r + t(x-r))\)
<p>Even though the left side has other features at \(r\) and the right side has them varying along the path, <strong>it doesn&#39;t matter</strong> because there are no interaction terms&#33;</p>
<p>So for a <strong>fixed reference</strong> \(r\):</p>
\(\mathbb{E}_{S \sim \pi}[\text{marginal contribution}] \approx \phi_i(x,r) \cdot (x_i - r_i)\)
<h3 id="step_6_now_average_over_references"><a href="#step_6_now_average_over_references" class="header-anchor">Step 6: Now Average Over References</a></h3>
<p>The full Shapley formula has:</p>
\(\phi_i^{\text{Shapley}} = \mathbb{E}_{r \sim \mathcal{R}} \left[ \mathbb{E}_{S \sim \pi}[\text{marginal contribution w.r.t. } r] \right]\)
<p>From Step 5, the inner expectation gives \(\phi_i(x,r) \cdot (x_i - r_i)\), so:</p>
\(\phi_i^{\text{Shapley}} \approx \mathbb{E}_{r \sim \mathcal{R}} [\phi_i(x,r) \cdot (x_i - r_i)]\)
<p><strong>This is what DeepSHAP computes&#33;</strong> Sample multiple references \(r^{(1)}, \ldots, r^{(K)}\), compute DeepLIFT for each, and average:</p>
\(\phi_i^{\text{DeepSHAP}} = \frac{1}{K} \sum_{k=1}^K \phi_i(x, r^{(k)}) \cdot (x_i - r_i^{(k)})\)
<h2 id="the_concrete_intuition"><a href="#the_concrete_intuition" class="header-anchor">The Concrete Intuition</a></h2>
<p>Think of it this way: </p>
<p><strong>Without independence</strong> &#40;features interact&#41;:</p>
<ul>
<li><p>Coalition \(S = \emptyset\): Adding \(x_1\) to \((r_1, r_2, r_3)\) has marginal effect \(A\)</p>
</li>
<li><p>Coalition \(S = \{2\}\): Adding \(x_1\) to \((r_1, x_2, r_3)\) has marginal effect \(B\) ≠ \(A\) &#40;different because \(x_2\) changes the gradient&#33;&#41;</p>
</li>
<li><p>Coalition \(S = \{3\}\): Adding \(x_1\) to \((r_1, r_2, x_3)\) has marginal effect \(C\) ≠ \(A\)</p>
</li>
<li><p>Coalition \(S = \{2,3\}\): Adding \(x_1\) to \((r_1, x_2, x_3)\) has marginal effect \(D\) ≠ \(A\)</p>
</li>
<li><p><strong>Shapley value</strong> &#61; \(\mathbb{E}_{S}[\text{marginal}]\) &#61; properly weighted average of \(A, B, C, D\) over all \(2^{n-1}\) coalitions</p>
</li>
<li><p><strong>You MUST compute all of them and average&#33;</strong></p>
</li>
</ul>
<p><strong>With independence</strong> &#40;no interactions&#41;:</p>
<ul>
<li><p>Coalition \(S = \emptyset\): Adding \(x_1\) to \((r_1, r_2, r_3)\) has marginal effect \(A\)</p>
</li>
<li><p>Coalition \(S = \{2\}\): Adding \(x_1\) to \((r_1, x_2, r_3)\) has marginal effect \(A\) &#40;same&#33; \(x_2\) doesn&#39;t change the gradient&#41;</p>
</li>
<li><p>Coalition \(S = \{3\}\): Adding \(x_1\) to \((r_1, r_2, x_3)\) has marginal effect \(A\) &#40;still same&#33;&#41;</p>
</li>
<li><p>Coalition \(S = \{2,3\}\): Adding \(x_1\) to \((r_1, x_2, x_3)\) has marginal effect \(A\) &#40;all equal&#33;&#41;</p>
</li>
<li><p><strong>Shapley value</strong> &#61; \(\mathbb{E}_{S}[\text{marginal}]\) &#61; average of \([A, A, A, A, \ldots]\) &#61; \(A\)</p>
</li>
<li><p><strong>The coalition expectation is still there, it just equals any single term&#33;</strong></p>
</li>
</ul>
<p>And DeepLIFT&#39;s integral along the full path also gives us \(A\) because the gradient doesn&#39;t depend on where other features are.</p>
<p><strong>So to directly answer your question:</strong> Yes, exactly&#33; Under independence, computing the coalition expectation reduces to a <strong>single calculation</strong> rather than an average over many terms. </p>
<p>Mathematically, you&#39;re still computing \(\mathbb{E}_S[\text{marginal}]\), but since every term in that expectation is identical, you only need to evaluate one of them:</p>
\(\mathbb{E}_S[\text{marginal}] = A \text{ (when all marginal contributions equal } A)\)
<p>It&#39;s the difference between:</p>
<ul>
<li><p><strong>Hard problem</strong>: &quot;Compute the average of 1000 different numbers&quot; &#40;needs all 1000 evaluations&#41;</p>
</li>
<li><p><strong>Easy problem</strong>: &quot;Compute the average of &#91;7, 7, 7, ..., 7&#93; &#40;1000 times&#41;&quot; &#40;just evaluate 7 once&#41;</p>
</li>
</ul>
<p>You&#39;re right to think of it as &quot;reducing to a single calculation&quot; rather than needing to actually compute and average many terms. The expectation operator is still there in the formula, but it degenerates to just picking out any representative term &#40;they&#39;re all the same&#41;.</p>
<p>That&#39;s the computational magic: independence transforms the intractable Shapley coalition average into a single path integral that DeepLIFT can compute in one backward pass.</p>
<h2 id="why_approximation"><a href="#why_approximation" class="header-anchor">Why &quot;Approximation&quot;?</a></h2>
<p>So when do we have:</p>
\[\mathbb{E}_{S}[\text{marginal contribution of } i \text{ in coalition } S] \approx \phi_i(x,r) \cdot (x_i - r_i)\]
<p><strong>Exactly true when:</strong></p>
<ol>
<li><p>Model is linear: \(f(x) = \sum_i \beta_i x_i\)</p>
</li>
<li><p>Features are independent: \(\frac{\partial f}{\partial x_i}\) doesn&#39;t depend on \(x_j\)</p>
</li>
</ol>
<p><strong>Approximately true when:</strong></p>
<ol>
<li><p>Model is &quot;locally linear&quot; &#40;smooth&#41;</p>
</li>
<li><p>Features have weak interactions</p>
</li>
<li><p>Reference distribution is reasonable</p>
</li>
</ol>
<p><strong>Fails when:</strong></p>
<ol>
<li><p>Strong feature interactions &#40;e.g., &quot;not good&quot; in text&#41;</p>
</li>
<li><p>Highly nonlinear regions</p>
</li>
<li><p>Unrealistic reference distribution</p>
</li>
</ol>
<h2 id="the_multiple_backprop_confusion_resolved"><a href="#the_multiple_backprop_confusion_resolved" class="header-anchor">The Multiple Backprop Confusion Resolved</a></h2>
<p>You asked: &quot;When we do multiple backprop, we are only approximating one marginal contribution term. How is that approximating the Shapley value?&quot;</p>
<p>Now we can answer precisely&#33; Each backprop with reference \(r^{(k)}\) gives:</p>
\(\phi_i(x, r^{(k)}) \cdot (x_i - r_i^{(k)})\)
<p>Under independence, this approximates:</p>
\(\mathbb{E}_{S \sim \pi}[\text{marginal contribution of } i \text{ in all coalitions w.r.t. reference } r^{(k)}]\)
<p>The <strong>multiple references</strong> give us different &quot;samples&quot; from the reference distribution \(\mathcal{R}\). By averaging:</p>
\(\frac{1}{K} \sum_{k=1}^K \phi_i(x, r^{(k)}) \cdot (x_i - r_i^{(k)})\)
<p>we&#39;re approximating:</p>
\(\mathbb{E}_{r \sim \mathcal{R}} \left[ \mathbb{E}_{S \sim \pi}[\text{marginal contribution}] \right]\)
<p>which is exactly the Shapley value&#33;</p>
<p><strong>Key insight:</strong> Each single backprop with one reference already approximates the entire coalition average &#40;inner expectation&#41; under independence. The multiple backprops then approximate the outer expectation over references.</p>
<h2 id="the_bottom_line"><a href="#the_bottom_line" class="header-anchor">The Bottom Line</a></h2>
<p>The independence assumption is doing <strong>massive</strong> heavy lifting:</p>
<ol>
<li><p>It makes all \(2^{n-1}\) coalition-specific marginal contributions equal</p>
</li>
<li><p>It makes the single-path integral &#40;DeepLIFT&#41; equal to the many-coalition average &#40;Shapley&#41;</p>
</li>
<li><p>It allows one backprop to capture what would require exponential computation</p>
</li>
</ol>
<p>Without independence, DeepSHAP is just a fast heuristic approximation. With independence, it&#39;s theoretically justified - but independence rarely holds in practice&#33;</p>
<p>That&#39;s why the papers say &quot;approximate SHAP values&quot; and why it works &quot;well enough&quot; empirically despite violating the theoretical assumptions.</p>
<p>Does this make the connection clearer?</p>
<div class="page-foot">
    Contact me by <a href="mailto:skchu@wustl.edu">E-mail</a> | <a href="https://github.com/kchu25">Github</a> | <a href="https://www.linkedin.com/in/kchu1/">Linkedin</a>
    <br>
    This work is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.  Last modified: December 20, 2025.
    <br>
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
  </body>
</html>
